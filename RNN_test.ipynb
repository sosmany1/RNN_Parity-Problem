{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sosmany1/RNN_Parity-Problem/blob/master/RNN_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_cBcjTnzH4X",
        "colab_type": "code",
        "outputId": "d9c99b8e-1567-49d0-acd4-da8905442f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        }
      },
      "source": [
        "#The input to the RNN at every time-step is the current value as well as a state vector \n",
        "#which represents what the network has “seen” at time-steps before. \n",
        "#This state-vector is the encoded memory of the RNN, initially set to zero.\n",
        "\n",
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_epochs = 100\n",
        "total_series_length = 50000\n",
        "truncated_backprop_length = 2\n",
        "#this is used in a confusing way, it basically refers to how many elements are inside each batch. \n",
        "#So the matrix we put in the placeholderX, has dimensions batchsize, aka number of rows, and each row then takes in certain number of elements which\n",
        "# here are referred to as truncated back prop length. go figure.\n",
        "state_size = 4\n",
        "num_classes = 2\n",
        "echo_step = 1\n",
        "batch_size = 4\n",
        "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
        "\n",
        "def generateData():\n",
        "    x = 1.0-2*(np.random.randn(batch_size,3) < 0).astype(np.float32)\n",
        "    y  = np.prod(x, axis=1)[:,np.newaxis]\n",
        "\n",
        "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
        "    y = y.reshape((batch_size, -1))\n",
        "    # Reshaping takes the whole dataset and puts it into a matrix, that later will be sliced up into mini-batches.\n",
        "    # reshape takes your data, and it reshapes it into a tensor with the shape (a,b). \n",
        "    #Here batch_size is the number of rows, and -1 means however many elements need to be in each row to fit all the data in batchsize num of rows.\n",
        "    \n",
        "    print(x)\n",
        "    print(y)\n",
        "    return (x, y)\n",
        "\n",
        "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length]) #placeholders are starting nodes, the first paramater gives datatype\n",
        "batchY_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length]) # the second gives shape, here its 5, 15. so matrix is 5 categories with 15 categories in them, you get the idea...\n",
        "\n",
        "init_state = tf.placeholder(tf.float32, [batch_size, state_size]) #here you have a placeholder for the initial state. they have the shape, 5 and 4.\n",
        "#states are fed into the next 'layer'. the placeholder for the state matrix is batch size, ie 5, and state size, ie 4. Check this \n",
        "\n",
        "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
        "b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
        "\n",
        "W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
        "b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)\n",
        "\n",
        "# Unpack columns\n",
        "inputs_series = tf.unstack(batchX_placeholder, axis=1) \n",
        "labels_series = tf.unstack(batchY_placeholder, axis=1) \n",
        "#here we are taking the columns of the matrix and putting them into rows so that we have a list (axis=1) of elements corresponding to diff batches\n",
        "#this is so that we can train on multiple batches at the same time.\n",
        "\n",
        "#now this also means that we have to save three (batch size) number of states. \n",
        "#So thats why the it state place holder defined above has a shape of batchsize (with statesize entries in it. not sure why this is the case)\n",
        "# Forward pass\n",
        "current_state = init_state\n",
        "states_series = []\n",
        "\n",
        "for current_input in inputs_series:\n",
        "    current_input = tf.reshape(current_input, [batch_size, 1])\n",
        "    input_and_state_concatenated = tf.concat([current_input, current_state],1)  # Increasing number of columns\n",
        "    # Notice the concatenation on above line, what we actually want to do is calculate the sum of two affine transforms:\n",
        "    #current_input * Wa + current_state * Wb as shown in the figure. \n",
        "    #By concatenating those two tensors you will only use one matrix multiplication. \n",
        "    #The addition of the bias b is broadcasted on all samples in the batch.r\n",
        "\n",
        "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
        "    states_series.append(next_state)\n",
        "    current_state = next_state\n",
        "\n",
        "print(\"states_series\")\n",
        "print(states_series)\n",
        "\n",
        "\n",
        "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
        "#W2 and B2 are the weights and biases of the state vector, they have their own weights because they act separately and propogate.\n",
        "#this gives a series as well because remember we are runnigng different parts of the time series at the same time.\n",
        "#this matrix with matmul states series and w2, is called \"logits series\". its just the output of states multiplied with their weights plus bias\n",
        "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
        "#print(logits_series)\n",
        "#print(labels_series)\n",
        "#sh = len(logits_series)\n",
        "#print(sh)\n",
        "#losses = [tf.reduce_mean((labels - logits)) for logits, labels in zip(logits_series,labels_series)]\n",
        "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels,logits) for logits, labels in zip(logits_series,labels_series)]\n",
        "total_loss = tf.reduce_mean(losses)\n",
        "\n",
        "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)\n",
        "\n",
        "def plot(loss_list, predictions_series, batchX, batchY):\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.cla()\n",
        "    plt.plot(loss_list)\n",
        "\n",
        "    for batch_series_idx in range(5):\n",
        "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
        "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
        "\n",
        "        plt.subplot(2, 3, batch_series_idx + 2)\n",
        "        plt.cla()\n",
        "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
        "        left_offset = range(truncated_backprop_length)\n",
        "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
        "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
        "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
        "\n",
        "    plt.draw()\n",
        "    plt.pause(0.0001)\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "    plt.ion()\n",
        "    plt.figure()\n",
        "    plt.show()\n",
        "    loss_list = []\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        x,y = generateData()\n",
        "        _current_state = np.zeros((batch_size, state_size))\n",
        "\n",
        "        print(\"New data, epoch\", epoch_idx)\n",
        "\n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * truncated_backprop_length\n",
        "            end_idx = start_idx + truncated_backprop_length\n",
        "\n",
        "            batchX = x[:,start_idx:end_idx]\n",
        "            batchY = y[:,start_idx:end_idx]\n",
        "\n",
        "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
        "                [total_loss, train_step, current_state, predictions_series],\n",
        "                feed_dict={\n",
        "                    batchX_placeholder:batchX,\n",
        "                    batchY_placeholder:batchY,\n",
        "                    init_state:_current_state\n",
        "                })\n",
        "\n",
        "            loss_list.append(_total_loss)\n",
        "\n",
        "            if batch_idx%100 == 0:\n",
        "                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
        "                plot(loss_list, _predictions_series, batchX, batchY)\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()\n",
        "\n",
        "print (\"inputs_series\")\n",
        "print(inputs_series)\n",
        "for input in inputs_series:\n",
        "  print(input)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "states_series\n",
            "[<tf.Tensor 'Tanh:0' shape=(4, 4) dtype=float32>, <tf.Tensor 'Tanh_1:0' shape=(4, 4) dtype=float32>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2b1d496352bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#print(sh)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m#losses = [tf.reduce_mean((labels - logits)) for logits, labels in zip(logits_series,labels_series)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_series\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_series\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-2b1d496352bc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#print(sh)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m#losses = [tf.reduce_mean((labels - logits)) for logits, labels in zip(logits_series,labels_series)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_series\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_series\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m   2023\u001b[0m   \"\"\"\n\u001b[1;32m   2024\u001b[0m   _ensure_xent_args(\"sparse_softmax_cross_entropy_with_logits\", _sentinel,\n\u001b[0;32m-> 2025\u001b[0;31m                     labels, logits)\n\u001b[0m\u001b[1;32m   2026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2027\u001b[0m   \u001b[0;31m# TODO(pcmurray) Raise an error when the label is not an index in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[0;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[1;32m   1773\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[0;32m-> 1775\u001b[0;31m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[1;32m   1776\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Only call `sparse_softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOBiHXytzOsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6lAdhpnjYB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}